{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31896af0-43e2-49e8-b246-d1598d5c41ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import *\n",
    "import numpy as np\n",
    "import h5py\n",
    "from Data_Core.experiment import *\n",
    "from Data_Core.digital_twin import *\n",
    "from tqdm import tqdm\n",
    "#from src.algorithms import *\n",
    "#import torch\n",
    "import sklearn\n",
    "%load_ext autoreload\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8642617c-a1ae-4943-89ab-696753be1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "def initialize(X, num_clusters, seed):\n",
    "    \"\"\"\n",
    "    Initialize cluster centers\n",
    "    \"\"\"\n",
    "\n",
    "    num_samples = len(X)\n",
    "    if seed == None:\n",
    "        indices = np.random.choice(num_samples, num_clusters, replace = False)\n",
    "    else:\n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.choice(num_samples, num_clusters, replace = False)\n",
    "    initial_state = X[indices]\n",
    "    return initial_state\n",
    "\n",
    "\n",
    "def kmeans_t(\n",
    "        X, \n",
    "        num_clusters,\n",
    "        distance = 'euclidean',\n",
    "        converg='cross_entropy',\n",
    "        cluster_centers = [],\n",
    "        tol = 1e-4,\n",
    "        tqdm_flag = True,\n",
    "        iter_limit = 20,\n",
    "        device = torch.device('cpu'),\n",
    "        seed = None\n",
    "):\n",
    "    \"\"\"\n",
    "    perform kmeans\n",
    "    \"\"\"\n",
    "    if tqdm_flag:\n",
    "        print(f'Running Kmeans on {device}..')\n",
    "    \n",
    "    if distance == 'euclidean':\n",
    "        pairwise_distance_function = partial(pairwise_distance, device = device, tqdm_flag = tqdm_flag)\n",
    "    elif distance == 'iou':\n",
    "        pairwise_distance_function = partial(pairwise_iou, device = device, tqdm_flag = tqdm_flag)\n",
    "    elif distance == 'cross_entropy':\n",
    "        pairwise_distance_function = partial(cross_entropy_loss, device = device, tqdm_flag = tqdm_flag)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Convert to Float\n",
    "    X = X.float()\n",
    "\n",
    "    # Transfer to Device\n",
    "    X = X.to(device)\n",
    "\n",
    "    if type(cluster_centers) == list:\n",
    "        initial_state = initialize(X, num_clusters, seed = seed)\n",
    "    else:\n",
    "        if tqdm_flag:\n",
    "            print('Resuming')\n",
    "\n",
    "        # Find closest point to initial cluster center\n",
    "        initial_state = cluster_centers\n",
    "\n",
    "        print('X',X)\n",
    "        print('initial state',initial_state)\n",
    "\n",
    "        \n",
    "        dis = pairwise_distance_function(X, initial_state)\n",
    "\n",
    "        choice_points = torch.argmin(dis, dim = 0)\n",
    "        initial_state = X[choice_points]\n",
    "        initial_state = initial_state.to(device)\n",
    "\n",
    "    iteration = 0\n",
    "    if tqdm_flag:\n",
    "        tqdm_meter = tqdm(desc = '[Running Kmeans]')\n",
    "    while True:\n",
    "\n",
    "        dis = pairwise_distance_function(X, initial_state)\n",
    "        choice_clusters = torch.argmin(dis, dim = 1)\n",
    "        initial_state_pre = initial_state.clone()\n",
    "\n",
    "        for index in range(num_clusters):\n",
    "            selected = torch.nonzero(choice_clusters == index).squeeze().to(device)\n",
    "            selected = torch.index_select(X, 0, selected)\n",
    "\n",
    "            if selected.shape[0] == 0:\n",
    "                selected = X[torch.randint(len(X), (1,))]\n",
    "\n",
    "            initial_state[index] = selected.mean(dim = 0)\n",
    "        \n",
    "        center_shift = 0\n",
    "        center_intersect = 0\n",
    "\n",
    "        if converg == 'euclidean':\n",
    "            center_shift = torch.sum(\n",
    "            torch.sqrt(\n",
    "                torch.sum((initial_state - initial_state_pre)**2, dim = 1)\n",
    "            ))\n",
    "        if converg=='cross_entropy':\n",
    "            center_shift=cross_entropy_loss(initial_state,initial_state_pre)\n",
    "\n",
    "            \n",
    "            \n",
    "        # if distance == 'iou':\n",
    "        #     inter = torch.sum((initial_state.int() & initial_state_pre.int()), dim = 1)\n",
    "        #     union = torch.sum((initial_state.int() | initial_state_pre.int()), dim = 1)\n",
    "\n",
    "        #     center_intersect = 1 - inter/union\n",
    "\n",
    "        # Increment iteration\n",
    "        iteration += 1\n",
    "        inertia = 0\n",
    "\n",
    "        # Update tqdm meter\n",
    "        if tqdm_flag:\n",
    "            tqdm_meter.set_postfix(\n",
    "                iteration = f'{iteration}',\n",
    "                center_shift = f'{center_shift**2:0.6f}',\n",
    "                tol = f'{tol:0.6f}'\n",
    "            )\n",
    "            tqdm_meter.update()\n",
    "\n",
    "        if distance == 'euclidean' and center_shift ** 2 < tol:\n",
    "            inertia = 0\n",
    "            for index in range(num_clusters):\n",
    "                selected = torch.nonzero(choice_clusters == index).squeeze().to(device)\n",
    "                selected = torch.index_select(X, 0, selected)\n",
    "                inertia += torch.sum(((selected - initial_state[index]) ** 2))\n",
    "            break\n",
    "        # if distance == 'iou' and center_intersect < tol:\n",
    "        #     break\n",
    "        if iter_limit != 0 and iteration >= iter_limit:\n",
    "            break\n",
    "\n",
    "    return choice_clusters.cpu(), initial_state.cpu(), inertia\n",
    "\n",
    "\n",
    "def kmeans_predict(\n",
    "        X,\n",
    "        cluster_center,\n",
    "        distance = 'euclidean',\n",
    "        device = torch.device('cpu'),\n",
    "        tqdm_flag = True\n",
    "):\n",
    "    # Predict using cluster centers\n",
    "\n",
    "    if tqdm_flag:\n",
    "        print(f'Predicting on {device}')\n",
    "\n",
    "    if distance == 'euclidean':\n",
    "        pairwise_distance_function = partial(pairwise_distance, device = device, tqdm_flag = tqdm_flag)\n",
    "    elif distance == 'iou':\n",
    "        pairwise_distance_function = partial(pairwise_iou, device = device, tqdm_flag = tqdm_flag)\n",
    "    elif distance == 'cross_entropy':\n",
    "        pairwise_distance_function = partial(cross_entropy_loss, device = device, tqdm_flag = tqdm_flag)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # Convert to Float\n",
    "    X = X.float()\n",
    "\n",
    "    # Transfer to Device\n",
    "    X = X.to(device)\n",
    "\n",
    "    dis = pairwise_distance_function(X, cluster_center)\n",
    "    choice_clusters = torch.argmin(dis, dim = 1)\n",
    "\n",
    "    return choice_clusters.cpu()\n",
    "\n",
    "\n",
    "def pairwise_distance(data1, data2, device = torch.device('cpu'), tqdm_flag = True):\n",
    "    # if tqdm_flag:\n",
    "    #     print(f'device is :{device}')\n",
    "    \n",
    "    # transfer to device\n",
    "    data1, data2 = data1.to(device), data2.to(device)\n",
    "\n",
    "    # N*1*M\n",
    "    A = data1.unsqueeze(dim = 1)\n",
    "\n",
    "    # 1*N*M\n",
    "    B = data2.unsqueeze(dim = 0)\n",
    "\n",
    "    dis = (A - B) ** 2.0\n",
    "    # return N*N matrix for pairwise distance\n",
    "    dis = dis.sum(dim = -1).squeeze()\n",
    "    return dis\n",
    "\n",
    "def pairwise_iou(data1, data2, device = torch.device('cpu'), tqdm_flag = True):\n",
    "    # if tqdm_flag:\n",
    "    #     print(f'device is :{device}')\n",
    "    \n",
    "    # transfer to device\n",
    "    data1, data2 = data1.to(device), data2.to(device)\n",
    "\n",
    "    A = data1.int()\n",
    "    B = data2.int()\n",
    "\n",
    "        # N*1*M\n",
    "    A = A.unsqueeze(dim = 1)\n",
    "\n",
    "    # 1*N*M\n",
    "    B = B.unsqueeze(dim = 0)\n",
    "\n",
    "    inter = torch.sum((A & B), dim = 2)\n",
    "    inter2 = torch.sum((torch.logical_not(A) & torch.logical_not(B)), dim = 2)\n",
    "    union = torch.sum((A | B), dim = 2)\n",
    "    union2 = torch.sum((torch.logical_not(A) | torch.logical_not(B)), dim = 2)\n",
    "\n",
    "    #inter3 = torch.sum((torch.logical_not(A) & B), dim = 2)\n",
    "    #union3 = torch.sum((torch.logical_not(A) | B), dim = 2)\n",
    "    \n",
    "    dis = 1 - (inter/union )#* inter2/union2))\n",
    "    # return N*N matrix for pairwise distance\n",
    "    dis = dis.squeeze()\n",
    "    return dis\n",
    "def cross_entropy_loss(data1,data2,device = torch.device('cpu'), tqdm_flag = True):\n",
    "    data1, data2 = data1.to(device), data2.to(device)\n",
    "    import torch.nn.functional as F\n",
    "    import torch\n",
    "    softmax = F.softmax(data1.float(), dim=0)\n",
    "    data1=softmax\n",
    "    loss = 0\n",
    " \n",
    "    # Doing cross entropy Loss\n",
    "    for i in range(len(data1)):\n",
    "        # Here, the loss is computed using the\n",
    "        # above mathematical formulation.\n",
    "        loss = loss + (-1 * np.dot(data2[i],np.log(data1[i])))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc8da67d-f2b8-4e9c-8454-55ed15500166",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_index = lambda wavelenghts, w : np.argmin(np.abs(wavelenghts - w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b9af73d-da29-4fa0-850e-c8863cd855d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Raman_data_loader(filename):\n",
    "\n",
    "    with h5py.File(filename, 'a') as output_file:\n",
    "         \n",
    "        properties = output_file['properties']\n",
    "        \n",
    "        exp_properties = {'step_size' : np.array(properties['step_size'])[0],\n",
    "                          'speed' : np.array(properties['speed']),\n",
    "                          'n_points' : np.array(properties['n_points'])\n",
    "            }\n",
    "        \n",
    "        wavelengths = np.array(output_file['properties']['x_data'])\n",
    "        \n",
    "        \n",
    "        spot_numbers = [int(s.split('_')[-1]) for s in list(output_file['data'].keys()) if 'spot' in s ]\n",
    "        \n",
    "        Nx,Ny = output_file['properties']['n_points'][0], output_file['properties']['n_points'][1]\n",
    "        Nl = len(wavelengths)\n",
    "        spectral_signal = np.zeros([Nx,Ny,Nl])\n",
    "        \n",
    "        \n",
    "        for _i, spot_number in enumerate(spot_numbers):\n",
    "\n",
    "            ix, iy = int(spot_number//Ny), int(spot_number%Ny)\n",
    "            \n",
    "            spot = 'spot_'+str(spot_number)\n",
    "            data = np.array(output_file['data'][spot]['raw_data'])\n",
    "                      \n",
    "            spectral_signal[ix,iy,:] = data\n",
    "            \n",
    "\n",
    "    return spectral_signal, wavelengths, exp_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c6ccf2b-01a1-4692-a0fc-1dae04c5a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"2023913_1110.h5\"\n",
    "#filename = \"2023811_1451.h5\"\n",
    "spectrum_raman, wavelengths_raman, exp_properties = Raman_data_loader(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94cccdb8-f82f-417f-adcc-564fb037c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "def baseline_als_optimized(y, lam, p, niter = 10):\n",
    "    L = len(y)\n",
    "    D = sparse.diags([1, -2, 1],[0, -1, -2], shape = (L, L - 2))\n",
    "    D = lam * D.dot(D.transpose()) # Precompute this term since it does not depend on `w`\n",
    "    w = np.ones(L)\n",
    "    W = sparse.spdiags(w, 0, L, L)\n",
    "    for i in range(niter):\n",
    "        W.setdiag(w) # Do not create a new matrix, just update diagonal values\n",
    "        Z = W + D\n",
    "        z = spsolve(Z, w*y)\n",
    "        w = p * (y > z) + (1 - p) * (y < z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a925629-9d0a-4373-9774-827ee3a50b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [07:26<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "lam = 1e2\n",
    "p = 1e-1\n",
    "    \n",
    "pro_data = 1*spectrum_raman\n",
    "for i in tqdm(range(0, pro_data.shape[0])):\n",
    "    for j in range(0, pro_data.shape[1]):\n",
    "        # print(i,j,end='\\r')\n",
    "        spec = 1 * pro_data[i, j, :]\n",
    "        pro_data[i, j, :] = spec - baseline_als_optimized(spec, lam = lam, p = p, niter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73100328-843b-4355-8cd7-efaaa5861f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_data_norm=pro_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e640c861-03b4-44b1-bd60-5703ea05ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_l = 31\n",
    "offset_m = 328\n",
    "\n",
    "mask = pro_data_norm[:, :, offset_l:offset_m].reshape(pro_data.shape[0]*pro_data.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1724ac6c-1825-4f2a-9112-51a17fc99e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.27e-9\n",
    "\n",
    "mask_offset = np.array(mask)\n",
    "mask_t = np.array([(mask_offset[:, i])*((mask_offset[:, i] > threshold)) for i in range(mask_offset.shape[-1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "457d66ba-b6ec-4327-880a-8a9130662791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ref(mineral):\n",
    "    df = pd.read_csv(mineral + '_raman.txt',header=13,names=['wavenumber','int'],skipfooter=4,engine='python' )\n",
    "    return np.array(df['wavenumber']),np.array(df['int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8af361b-8ee4-432c-9cdd-d451ad843de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(mask_t)\n",
    "mask_min_max=scaler.transform(mask_t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61931a87-aaa4-480d-bdd2-4750470b3646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "673eb65b-dc9c-41bb-b888-acb4c71a1180",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_masks=mask_min_max*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2085e8c4-d528-4950-a8d5-ea1b1f6d03bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_calc(map_gr):\n",
    "    import cv2\n",
    "    from scipy.stats import entropy\n",
    "\n",
    "    #image\n",
    "\n",
    "    _bins = 200\n",
    "\n",
    "    hist, _ = np.histogram(map_gr, bins=_bins, range=(0, _bins))\n",
    "\n",
    "    prob_dist = hist / hist.sum()\n",
    "    image_entropy = entropy(prob_dist, base=2)\n",
    "    return image_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bf1943a-19d5-4b26-a5f3-73e3d233e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_l = 31\n",
    "offset_m = 328\n",
    "wavelengths_raman.shape\n",
    "wavelengths=wavelengths_raman[offset_l:offset_m]\n",
    "\n",
    "minerals = {\n",
    "    \n",
    "    'Albite':[508.1],\n",
    "    'Background':[733.1],\n",
    "    'Petalite':[491.3],\n",
    "    'Quartz':[463.92],\n",
    "    'Spodumene':[704.5]}\n",
    "\n",
    "mineral_list = list(minerals.keys())\n",
    "\n",
    "find_index = lambda wavelenghts,w : np.argmin(np.abs(wavelengths-w))\n",
    "\n",
    "significant_w=[]\n",
    "for i,mineral in enumerate(mineral_list):\n",
    "    wn=find_index(wavelengths,minerals[mineral][0])\n",
    "    significant_w.append(wn)\n",
    "\n",
    "\n",
    "image_entropy=[]\n",
    "for i in range(297):\n",
    "    map_gr=entropy_masks[i]\n",
    "    image_entropy.append(entropy_calc(map_gr))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05e7b48f-f38b-4914-aef6-f6d2b6a1677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_entropies=[]\n",
    "for i in significant_w:\n",
    "    relevant_entropies.append(image_entropy[i])\n",
    "    \n",
    "max_en=np.max(relevant_entropies)+0.2\n",
    "min_en=np.min(relevant_entropies)-0.2\n",
    "#min_en=5.2\n",
    "#max_en=6.3\n",
    "\n",
    "new_maps_cut=[]\n",
    "for i in range(entropy_masks.shape[0]):\n",
    "    if min_en<=image_entropy[i]<=max_en:\n",
    "        new_maps_cut.append(entropy_masks[i,:])\n",
    "        \n",
    "new_maps_cut=np.array(new_maps_cut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "938ad419-ed7e-49e8-a8fa-9c2a59927e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_torch = torch.from_numpy(new_maps_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64c339af-7f4b-49e7-9334-ccce315e1dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([275, 25000])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f90426cd-9f52-4dae-9884-1a38ced83bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Kmeans on cpu..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Running Kmeans]: 0it [00:00, ?it/s]C:\\Users\\Joana\\AppData\\Local\\Temp\\ipykernel_20160\\1709818985.py:229: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = loss + (-1 * np.dot(data2[i],np.log(data1[i])))\n",
      "[Running Kmeans]: 20it [00:03,  6.56it/s, center_shift=nan, iteration=20, tol=0.000100]\n"
     ]
    }
   ],
   "source": [
    "kmeans_cpu = kmeans_t(X = mask_torch, num_clusters = 5, distance = 'euclidean',converg='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9f4ded91-3d51-485d-a207-97c98cbe4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(data1,data2,device = torch.device('cpu'), tqdm_flag = True):\n",
    "    data1, data2 = data1.to(device), data2.to(device)\n",
    "    import torch.nn.functional as F\n",
    "    import torch\n",
    "    softmax = F.softmax(data1.float(), dim=0)\n",
    "    data1=softmax\n",
    "    loss = 0\n",
    " \n",
    "    # Doing cross entropy Loss\n",
    "    for i in range(len(data1)):\n",
    "        #print(f\"i: {i}, data1 size: {data1.size()}, data2 size: {data2.size()}\")\n",
    "        # Here, the loss is computed using the\n",
    "        # above mathematical formulation.\n",
    "        print('data1 of the i ',data1[i])\n",
    "        print('loss i', loss)\n",
    "        print('data 2', data2[i])\n",
    "        print('data1', data1[i])\n",
    "        print('softmax', softmax[i])\n",
    "        print('log',np.log(data1[i]))\n",
    "        print('dot',np.dot(data2[i],np.log(data1[i])))\n",
    "        \n",
    "        \n",
    "        loss = loss + (-1 * np.dot(data2[i],np.log(data1[i])))\n",
    "        if i>0:\n",
    "            break\n",
    "\n",
    "    return loss\n",
    "\n",
    "data1=torch.from_numpy(np.array([[0,2,3],\n",
    "                [1,2,3]]))\n",
    "data2=torch.from_numpy(np.array(np.array([[4,5,6],\n",
    "                [4,5,6]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3cccde81-162b-4a3a-8113-58aedb9c3bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1 of the i  tensor(0.)\n",
      "loss i 0\n",
      "data 2 tensor(14.1885, dtype=torch.float64)\n",
      "data1 tensor(0.)\n",
      "softmax tensor(0.)\n",
      "log tensor(-inf)\n",
      "dot -inf\n",
      "data1 of the i  tensor(0.)\n",
      "loss i inf\n",
      "data 2 tensor(0.3973, dtype=torch.float64)\n",
      "data1 tensor(0.)\n",
      "softmax tensor(0.)\n",
      "log tensor(-inf)\n",
      "dot -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joana\\AppData\\Local\\Temp\\ipykernel_20160\\972483849.py:19: RuntimeWarning: divide by zero encountered in log\n",
      "  print('log',np.log(data1[i]))\n",
      "C:\\Users\\Joana\\AppData\\Local\\Temp\\ipykernel_20160\\972483849.py:20: RuntimeWarning: divide by zero encountered in log\n",
      "  print('dot',np.dot(data2[i],np.log(data1[i])))\n",
      "C:\\Users\\Joana\\AppData\\Local\\Temp\\ipykernel_20160\\972483849.py:23: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = loss + (-1 * np.dot(data2[i],np.log(data1[i])))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(mask_torch[0],mask_torch[13])\n",
    "data1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d40375a6-6af8-4d9c-acbf-266fc006f010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3823e-112, 1.3823e-112, 7.6490e-109,  ..., 1.3418e-108,\n",
       "        1.3823e-112,  9.0682e-94], dtype=torch.float64)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1=np.array([[0,0.2,0.3],\n",
    "                [1,2,3]])\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "softmax = F.softmax(mask_torch[0], dim=0)\n",
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "64d5172d-3908-4025-8a2b-c3cba9ddb4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-257.56513075994616"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log(1.3832e-112)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
