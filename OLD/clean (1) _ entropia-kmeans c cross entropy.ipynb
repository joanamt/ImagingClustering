{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import *\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "#from src.algorithms import *\n",
    "#import torch\n",
    "import sklearn\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_index = lambda wavelenghts, w : np.argmin(np.abs(wavelenghts - w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Raman_data_loader(filename):\n",
    "\n",
    "    with h5py.File(filename, 'a') as output_file:\n",
    "         \n",
    "        properties = output_file['properties']\n",
    "        \n",
    "        exp_properties = {'step_size' : np.array(properties['step_size'])[0],\n",
    "                          'speed' : np.array(properties['speed']),\n",
    "                          'n_points' : np.array(properties['n_points'])\n",
    "            }\n",
    "        \n",
    "        wavelengths = np.array(output_file['properties']['x_data'])\n",
    "        \n",
    "        \n",
    "        spot_numbers = [int(s.split('_')[-1]) for s in list(output_file['data'].keys()) if 'spot' in s ]\n",
    "        \n",
    "        Nx,Ny = output_file['properties']['n_points'][0], output_file['properties']['n_points'][1]\n",
    "        Nl = len(wavelengths)\n",
    "        spectral_signal = np.zeros([Nx,Ny,Nl])\n",
    "        \n",
    "        \n",
    "        for _i, spot_number in enumerate(spot_numbers):\n",
    "\n",
    "            ix, iy = int(spot_number//Ny), int(spot_number%Ny)\n",
    "            \n",
    "            spot = 'spot_'+str(spot_number)\n",
    "            data = np.array(output_file['data'][spot]['raw_data'])\n",
    "                      \n",
    "            spectral_signal[ix,iy,:] = data\n",
    "            \n",
    "\n",
    "    return spectral_signal, wavelengths, exp_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"2023913_1110.h5\"\n",
    "#filename = \"2023811_1451.h5\"\n",
    "spectrum_raman, wavelengths_raman, exp_properties = Raman_data_loader(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "def baseline_als_optimized(y, lam, p, niter = 10):\n",
    "    L = len(y)\n",
    "    D = sparse.diags([1, -2, 1],[0, -1, -2], shape = (L, L - 2))\n",
    "    D = lam * D.dot(D.transpose()) # Precompute this term since it does not depend on `w`\n",
    "    w = np.ones(L)\n",
    "    W = sparse.spdiags(w, 0, L, L)\n",
    "    for i in range(niter):\n",
    "        W.setdiag(w) # Do not create a new matrix, just update diagonal values\n",
    "        Z = W + D\n",
    "        z = spsolve(Z, w*y)\n",
    "        w = p * (y > z) + (1 - p) * (y < z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████████████▌                                              | 105/250 [04:48<05:54,  2.44s/it]"
     ]
    }
   ],
   "source": [
    "lam = 1e2\n",
    "p = 1e-1\n",
    "\n",
    "pro_data = 1*spectrum_raman\n",
    "for i in tqdm(range(0, pro_data.shape[0])):\n",
    "    for j in range(0, pro_data.shape[1]):\n",
    "        # print(i,j,end='\\r')\n",
    "        spec = 1 * pro_data[i, j, :]\n",
    "        pro_data[i, j, :] = spec - baseline_als_optimized(spec, lam = lam, p = p, niter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_data_norm=pro_data.copy()\n",
    "for i in range(0,pro_data.shape[0]):\n",
    "    for j in range(0,pro_data.shape[1]):\n",
    "        spec = 1 * pro_data[i,j, :]\n",
    "        sum_spec=np.sum(spec)\n",
    "        pro_data_norm[i,j,:]=spec/sum_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_l = 31\n",
    "offset_m = 328\n",
    "\n",
    "mask = pro_data_norm[:, :, offset_l:offset_m].reshape(pro_data.shape[0]*pro_data.shape[1], -1)\n",
    "\n",
    "# threshold = 120\n",
    "# threshold2 = 0.01\n",
    "\n",
    "# # Scaled to max value for each wavenumber\n",
    "# # masks = np.array([mask[:, i]*(mask[:, i] > threshold)/np.max(mask[:, i]) for i in range(mask.shape[-1])])\n",
    "# masks = np.array([np.array((mask[:, i]/np.max(mask[:, i]))*((mask[:, i] > threshold)) > threshold2, dtype = int) for i in range(mask.shape[-1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.27e-9\n",
    "\n",
    "mask_offset = np.array(mask)\n",
    "mask_t = np.array([(mask_offset[:, i])*((mask_offset[:, i] > threshold)) for i in range(mask_offset.shape[-1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_k=[0,1]\n",
    "mask_min_max = np.array([(mask_t[i, :] - mask_t[i, :].min(axis = 0)+ 1e-5)/(mask_t[i, :].max(axis = 0) - mask_t[i, :].min(axis = 0) + 1e-5) for i in range(mask_t.shape[0])])\n",
    "# mask_min_max = np.array([(mask_t[i, :])/(mask_t[i, :].max(axis = 0) + 1e-10) for i in range(mask_t.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ref(mineral):\n",
    "    df = pd.read_csv(mineral + '_raman.txt',header=13,names=['wavenumber','int'],skipfooter=4,engine='python' )\n",
    "    return np.array(df['wavenumber']),np.array(df['int'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo da entropia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mask_min_max.shape)\n",
    "entropy_masks=np.array([mask_min_max[i,:]*255 for i in range(297)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_masks[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_calc(map_gr):\n",
    "    import cv2\n",
    "    from scipy.stats import entropy\n",
    "\n",
    "    #image\n",
    "\n",
    "    _bins = 200\n",
    "\n",
    "    hist, _ = np.histogram(map_gr, bins=_bins, range=(0, _bins))\n",
    "\n",
    "    prob_dist = hist / hist.sum()\n",
    "    image_entropy = entropy(prob_dist, base=2)\n",
    "    return image_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_l = 31\n",
    "offset_m = 328\n",
    "wavelengths_raman.shape\n",
    "wavelengths=wavelengths_raman[offset_l:offset_m]\n",
    "\n",
    "minerals = {\n",
    "    \n",
    "    'Albite':[508.1],\n",
    "    'Background':[733.1],\n",
    "    'Petalite':[491.3],\n",
    "    'Quartz':[463.92],\n",
    "    'Spodumene':[704.5]}\n",
    "\n",
    "mineral_list = list(minerals.keys())\n",
    "\n",
    "find_index = lambda wavelenghts,w : np.argmin(np.abs(wavelengths-w))\n",
    "\n",
    "significant_w=[]\n",
    "for i,mineral in enumerate(mineral_list):\n",
    "    wn=find_index(wavelengths,minerals[mineral][0])\n",
    "    significant_w.append(wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_entropy=[]\n",
    "for i in range(297):\n",
    "    map_gr=entropy_masks[i]\n",
    "    image_entropy.append(entropy_calc(map_gr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(image_entropy,bins=20)\n",
    "colors=['red','pink','black','green','brown']\n",
    "for i in range(len(significant_w)):\n",
    "    plt.vlines(image_entropy[significant_w[i]],0,40,color=colors[i],label=mineral_list[i])\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropias intermédias é a que mantém mais informação - ajustar a gama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(image_entropy,bins=20)\n",
    "colors=['red','pink','black','green','brown']\n",
    "for i in range(len(significant_w)):\n",
    "    plt.vlines(image_entropy[significant_w[i]],0,40,color=colors[i],label=mineral_list[i])\n",
    "    \n",
    "plt.xticks([3,4,5,5.2,5.4,5.6,5.8,6])\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_entropies=[]\n",
    "for i in significant_w:\n",
    "    relevant_entropies.append(image_entropy[i])\n",
    "    \n",
    "#max_en=np.max(relevant_entropies)\n",
    "#min_en=np.min(relevant_entropies)\n",
    "min_en=5.0\n",
    "max_en=6.0\n",
    "\n",
    "new_maps_cut=[]\n",
    "for i in range(entropy_masks.shape[0]):\n",
    "    if min_en<=image_entropy[i]<=max_en:\n",
    "        new_maps_cut.append(entropy_masks[i,:])\n",
    "        \n",
    "new_maps_cut=np.array(new_maps_cut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters = 5)\n",
    "a=kmeans.fit_predict(new_maps_cut)\n",
    "labels = kmeans.labels_\n",
    "fig, ax = subplots(2, 3, figsize = (10, 8))\n",
    "# clusters_c = np.array(kmeans.cluster_centers_.cpu())\n",
    "clusters_c = np.array(kmeans.cluster_centers_)\n",
    "\n",
    "j = 0\n",
    "l = 0\n",
    "\n",
    "for i in range(len(np.unique(labels))):\n",
    "    axs = ax[l, j]\n",
    "    # axs.imshow(clusters_c[i].reshape(pro_data.shape[:2]).T, interpolation = 'none')\n",
    "    axs.imshow(clusters_c[i].reshape(pro_data.shape[0], pro_data.shape[1]).T, interpolation = 'none')\n",
    "    axs.set_ylabel('y (mm)')\n",
    "    axs.set_xlabel('x (mm)')\n",
    "    axs.grid(False)\n",
    "    j += 1\n",
    "    if j == 3:\n",
    "        j = 0\n",
    "        l += 1\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps=new_maps_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(maps.shape)\n",
    "print(maps[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P=maps.mean(axis=0)\n",
    "#P=np.random.random(25000)*255\n",
    "P=maps[significant_w[3]]\n",
    "P.sort()\n",
    "final_masks=[]\n",
    "for i in range(maps.shape[0]):\n",
    "    X=maps[i].copy()\n",
    "    #X.sort()\n",
    "    sorted_args=X.argsort()\n",
    "    new_X=np.ones(maps.shape[1])\n",
    "    for j in range(maps.shape[1]):\n",
    "        #new_X[j]=float(P[sorted_args[j]])\n",
    "        new_X[sorted_args[j]]=P[j]\n",
    "        \n",
    "    #print(new_X)\n",
    "    final_masks.append(new_X)\n",
    "final_masks=np.array(final_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entropy_calc(final_masks[12,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(final_masks[13].reshape(250,100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = sklearn.cluster.KMeans(n_clusters = 5)\n",
    "a=kmeans.fit_predict(final_masks)\n",
    "labels = kmeans.labels_\n",
    "fig, ax = subplots(2, 3, figsize = (10, 8))\n",
    "# clusters_c = np.array(kmeans.cluster_centers_.cpu())\n",
    "clusters_c = np.array(kmeans.cluster_centers_)\n",
    "\n",
    "j = 0\n",
    "l = 0\n",
    "\n",
    "for i in range(len(np.unique(labels))):\n",
    "    axs = ax[l, j]\n",
    "    # axs.imshow(clusters_c[i].reshape(pro_data.shape[:2]).T, interpolation = 'none')\n",
    "    axs.imshow(clusters_c[i].reshape(pro_data.shape[0], pro_data.shape[1]).T, interpolation = 'none')\n",
    "    axs.set_ylabel('y (mm)')\n",
    "    axs.set_xlabel('x (mm)')\n",
    "    axs.grid(False)\n",
    "    j += 1\n",
    "    if j == 3:\n",
    "        j = 0\n",
    "        l += 1\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cortar o background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps=new_maps_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_maps=maps.reshape(-1,250,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_maps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(reshaped_maps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=subplots(1,2)\n",
    "ax[0].imshow(reshaped_maps[13])\n",
    "#cortes\n",
    "maps_without_background=reshaped_maps[13,5:220,7:75]\n",
    "ax[1].imshow(maps_without_background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_without_background_1=reshaped_maps[:,5:220,7:75]\n",
    "maps_without_background=maps_without_background_1.reshape(maps_without_background_1.shape[0],maps_without_background_1.shape[1]*maps_without_background_1.shape[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = sklearn.cluster.KMeans(n_clusters = 5)\n",
    "a=kmeans.fit_predict(maps_without_background)\n",
    "labels = kmeans.labels_\n",
    "fig, ax = subplots(2, 3,figsize=(10,8))\n",
    "# clusters_c = np.array(kmeans.cluster_centers_.cpu())\n",
    "clusters_c = np.array(kmeans.cluster_centers_)\n",
    "\n",
    "j = 0\n",
    "l = 0\n",
    "\n",
    "for i in range(len(np.unique(labels))):\n",
    "    axs = ax[l, j]\n",
    "    # axs.imshow(clusters_c[i].reshape(pro_data.shape[:2]).T, interpolation = 'none')\n",
    "    axs.imshow(clusters_c[i].reshape(maps_without_background_1.shape[1], maps_without_background_1.shape[2]).T, interpolation = 'none')\n",
    "    axs.set_ylabel('y (mm)')\n",
    "    axs.set_xlabel('x (mm)')\n",
    "    axs.grid(False)\n",
    "    j += 1\n",
    "    if j == 3:\n",
    "        j = 0\n",
    "        l += 1\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classificação a usar todos os mapas com entropias equalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = sklearn.cluster.KMeans(n_clusters = 6)\n",
    "a=kmeans.fit_predict(final_masks)\n",
    "labels = kmeans.labels_\n",
    "fig, ax = subplots(2, 3, figsize = (10, 8))\n",
    "# clusters_c = np.array(kmeans.cluster_centers_.cpu())\n",
    "clusters_c = np.array(kmeans.cluster_centers_)\n",
    "\n",
    "j = 0\n",
    "l = 0\n",
    "\n",
    "for i in range(len(np.unique(labels))):\n",
    "    axs = ax[l, j]\n",
    "    # axs.imshow(clusters_c[i].reshape(pro_data.shape[:2]).T, interpolation = 'none')\n",
    "    axs.imshow(clusters_c[i].reshape(pro_data.shape[0], pro_data.shape[1]).T, interpolation = 'none')\n",
    "    axs.set_ylabel('y (mm)')\n",
    "    axs.set_xlabel('x (mm)')\n",
    "    axs.grid(False)\n",
    "    j += 1\n",
    "    if j == 3:\n",
    "        j = 0\n",
    "        l += 1\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cortar primeiro os mapas com entropias fora das zonas e depois fazer a equalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_entropies=[]\n",
    "for i in significant_w:\n",
    "    relevant_entropies.append(image_entropy[i])\n",
    "    \n",
    "max_en=np.max(relevant_entropies)\n",
    "min_en=np.min(relevant_entropies)\n",
    "\n",
    "new_maps_cut=[]\n",
    "for i in range(entropy_masks.shape[0]):\n",
    "    if min_en<=image_entropy[i]<=max_en:\n",
    "        new_maps_cut.append(entropy_masks[i,:])\n",
    "        \n",
    "new_maps_cut=np.array(new_maps_cut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P=new_maps_cut.mean(axis=0)\n",
    "P=np.random.random(25000)\n",
    "#sorted_args=P.argsort()\n",
    "P.sort()\n",
    "final_masks=[]\n",
    "for i in range(new_maps_cut.shape[0]):\n",
    "    X=new_maps_cut[i].copy()\n",
    "    #X.sort()\n",
    "    sorted_args=X.argsort()\n",
    "    new_X=np.ones(new_maps_cut.shape[1])\n",
    "    for j in range(new_maps_cut.shape[1]):\n",
    "        #new_X[j]=float(P[sorted_args[j]])\n",
    "        new_X[sorted_args[j]]=P[j]\n",
    "        \n",
    "    #print(new_X)\n",
    "    final_masks.append(new_X)\n",
    "    \n",
    "final_masks=np.array(final_masks)*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = sklearn.cluster.KMeans(n_clusters = 4)\n",
    "a=kmeans.fit_predict(final_masks)\n",
    "labels = kmeans.labels_\n",
    "fig, ax = subplots(2, 2, figsize = (10, 8))\n",
    "# clusters_c = np.array(kmeans.cluster_centers_.cpu())\n",
    "clusters_c = np.array(kmeans.cluster_centers_)\n",
    "\n",
    "j = 0\n",
    "l = 0\n",
    "\n",
    "for i in range(len(np.unique(labels))):\n",
    "    axs = ax[l, j]\n",
    "    # axs.imshow(clusters_c[i].reshape(pro_data.shape[:2]).T, interpolation = 'none')\n",
    "    axs.imshow(clusters_c[i].reshape(pro_data.shape[0], pro_data.shape[1]).T, interpolation = 'none')\n",
    "    axs.set_ylabel('y (mm)')\n",
    "    axs.set_xlabel('x (mm)')\n",
    "    axs.grid(False)\n",
    "    j += 1\n",
    "    if j == 2:\n",
    "        j = 0\n",
    "        l += 1\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
